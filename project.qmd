---
title: " Customer Segmentation using Machine Learning in R"
author: "JOHN ANAM"
format: docx
editor: visual
---

Customer Segmentation is one the most important applications of unsupervised learning. Using clustering techniques, companies can identify the several segments of customers allowing them to target the potential user base. In this machine learning project, we will make use of [***K-means clustering***](https://data-flair.training/blogs/k-means-clustering-tutorial/) which is the essential algorithm for clustering unlabeled dataset.

### What is Customer Segmentation?

*Customer Segmentation is the process of division of customer base into several groups of individuals that share a similarity in different ways that are relevant to marketing such as gender, age, interests, and miscellaneous spending habits.*

## How to Implement Customer Segmentation in R?

```{r warning=FALSE,message=FALSE}
library(readr)
Mall_Customers <- read_csv("C:/Users/HP/Downloads/Mall_Customers.csv")
str(Mall_Customers)
names(Mall_Customers)
```

```{r warning=FALSE, message=FALSE}
sd(Mall_Customers$Age)
summary(Mall_Customers$Annual.Income..k..)
sd(Mall_Customers$Annual.Income..k..)
summary(Mall_Customers$Age)
```

### Customer Gender Visualization

In this, we will create a barplot and a piechart to show the gender distribution across our customer_data dataset.

```{r}
a <- table(Mall_Customers$Gender)
barplot(a,main="Using BarPlot to display Gender Comparision",
       ylab="Count",
       xlab="Gender",
       col=rainbow(2),
       legend=rownames(a))
```

From the above barplot, we observe that the number of females is higher than the males. Now, let us visualize a pie chart to observe the ratio of male and female distribution.

```{r}
pct <- round(a/sum(a)*100)
lbs <- paste(c("Female","Male")," ",pct,"%",sep=" ")
library(plotrix)
pie3D(a,labels=lbs,
   main="Pie Chart Depicting Ratio of Female and Male")
```

From the above graph, we conclude that the percentage of females is **56%**, whereas the percentage of male in the customer dataset is **44%**.

### Visualization of Age Distribution

Let us plot a histogram to view the distribution to plot the frequency of customer ages. We will first proceed by taking summary of the Age variable.

```{r}
summary(Mall_Customers$Age)
```

```{r}
hist(Mall_Customers$Age,
    col="blue",
    main="Histogram to Show Count of Age Class",
    xlab="Age Class",
    ylab="Frequency",
    labels=TRUE)
```

### Analysis of the Annual Income of the Customers

```{r}
View(Mall_Customers)
```

```{r}
names(Mall_Customers)
```

In this section of the R project, we will create visualizations to analyze the annual income of the customers. We will plot a histogram and then we will proceed to examine this data using a density plot.

```{r}
# Summary of Annual Income
summary(Mall_Customers$`Annual Income (k$)`)

# Histogram for Annual Income
hist(Mall_Customers$`Annual Income (k$)`,
     col = "#660033",
     main = "Histogram for Annual Income",
     xlab = "Annual Income Class",
     ylab = "Frequency",
     labels = TRUE)

```

```{r warning=FALSE, message=FALSE}
plot(density(Mall_Customers$`Annual Income (k$)`),
    col="yellow",
    main="Density Plot for Annual Income",
    xlab="Annual Income Class",
    ylab="Density")
polygon(density(Mall_Customers$`Annual Income (k$)`),
        col="#ccff66")

```

From the above descriptive analysis, we conclude that the minimum annual income of the customers is 15 and the maximum income is 137. People earning an average income of 70 have the highest frequency count in our histogram distribution. The average salary of all the customers is 60.56. In the Kernel Density Plot that we displayed above, we observe that the annual income has a [***normal distribution***](https://data-flair.training/blogs/normal-distribution-in-r/).

## Analyzing Spending Score of the Customers

```{r}
summary(Mall_Customers$`Spending Score (1-100)`)

##Min. 1st Qu. Median Mean 3rd Qu. Max. 
## 1.00 34.75 50.00 50.20 73.00 99.00

boxplot(Mall_Customers$`Spending Score (1-100)`,
   horizontal=TRUE,
   col="#990000",
   main="BoxPlot for Descriptive Analysis of Spending Score")
```

```{r}
hist(Mall_Customers$`Spending Score (1-100)`,
    main="HistoGram for Spending Score",
    xlab="Spending Score Class",
    ylab="Frequency",
    col="#6600cc",
    labels=TRUE)
```

The minimum spending score is 1, maximum is 99 and the average is 50.20. We can see Descriptive Analysis of Spending Score is that Min is 1, Max is 99 and avg. is 50.20. From the histogram, we conclude that customers between class 40 and 50 have the highest spending score among all the classes.

## K-means Algorithm

```{r}
library(purrr)
set.seed(123)
# function to calculate total intra-cluster sum of square 
iss <- function(k) {
  kmeans(Mall_Customers[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}

k.values <- 1:10


iss_values <- map_dbl(k.values, iss)

plot(k.values, iss_values,
    type="b", pch = 19, frame = FALSE, 
    xlab="Number of clusters K",
    ylab="Total intra-clusters sum of squares")
```

#### Average Silhouette Method

With the help of the average silhouette method, we can measure the quality of our clustering operation. With this, we can determine how well within the cluster is the data object. If we obtain a high average silhouette width, it means that we have good clustering. The average silhouette method calculates the mean of silhouette observations for different k values. With the optimal number of k clusters, one can maximize the average silhouette over significant values for k clusters.

Using the silhouette function in the cluster package, we can compute the average silhouette width using the kmean function. Here, the optimal cluster will possess highest average.

```{r}
library(cluster) 
library(gridExtra)
library(grid)


k2<-kmeans(Mall_Customers[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(Mall_Customers[,3:5],"euclidean")))
```

```{r}
k3<-kmeans(Mall_Customers[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(Mall_Customers[,3:5],"euclidean")))
```

#### Gap Statistic Method

In 2001, researchers at Stanford University -- **R. Tibshirani, G.Walther and T. Hastie** published the Gap Statistic Method. We can use this method to any of the clustering method like K-means, hierarchical clustering etc. Using the gap statistic, one can compare the total intracluster variation for different values of k along with their expected values under the null reference distribution of data. With the help of **Monte Carlo simulations**, one can produce the sample dataset. For each variable in the dataset, we can calculate the range between min(xi) and max (xj) through which we can produce values uniformly from interval lower bound to upper bound.

For computing the gap statistics method we can utilize the clusGap function for providing gap statistic as well as standard error for a given output.

Now, let us take k = 6 as our optimal cluster

```{r}
k6<-kmeans(Mall_Customers[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6
```

In the output of our kmeans operation, we observe a list with several key information. From this, we conclude the useful information being --

-   **cluster --** This is a vector of several integers that denote the cluster which has an allocation of each point.

-   **totss --** This represents the total sum of squares.

-   **centers --** Matrix comprising of several cluster centers

-   **withinss --** This is a vector representing the intra-cluster sum of squares having one component per cluster.

-   **tot.withinss --** This denotes the total intra-cluster sum of squares.

-   **betweenss --** This is the sum of between-cluster squares.

-   **size --** The total number of points that each cluster holds.

    ## Visualizing the Clustering Results using the First Two Principle Components

    ```{r}
    pcclust=prcomp(Mall_Customers[,3:5],scale=FALSE) #principal component analysis
    summary(pcclust)

    pcclust$rotation[,1:2]
    ```

```{r}
library(ggplot2)
set.seed(1)
ggplot(Mall_Customers, aes(x =`Annual Income (k$)`, y = `Spending Score (1-100)`)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

```{r}
ggplot(Mall_Customers, aes(x =`Spending Score (1-100)`, y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                      breaks=c("1", "2", "3", "4", "5","6"),
                      labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

```{r}
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}

digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters

plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")
legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))
```

## Summary

In this data science project, I have gone through the customer segmentation model. I have developed this using a class of machine learning known as unsupervised learning. Specifically, I made use of a clustering algorithm called K-means clustering. I have analyzed and visualized the data and then proceeded to implement our algorithm. Hope you enjoyed this customer segmentation project of machine learning using R.
